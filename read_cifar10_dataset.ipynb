{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classname=truck\n",
      "classname=dog\n",
      "classname=frog\n",
      "classname=horse\n",
      "classname=ship\n",
      "classname=airplane\n",
      "classname=deer\n",
      "classname=bird\n",
      "classname=cat\n",
      "classname=automobile\n",
      "(array([[[ 192.,  216.,  241.],\n",
      "        [ 192.,  216.,  241.],\n",
      "        [ 192.,  217.,  242.],\n",
      "        ..., \n",
      "        [ 209.,  232.,  248.],\n",
      "        [ 211.,  233.,  249.],\n",
      "        [ 211.,  230.,  247.]],\n",
      "\n",
      "       [[ 192.,  216.,  239.],\n",
      "        [ 193.,  218.,  241.],\n",
      "        [ 193.,  217.,  240.],\n",
      "        ..., \n",
      "        [ 205.,  230.,  242.],\n",
      "        [ 202.,  228.,  242.],\n",
      "        [ 198.,  220.,  240.]],\n",
      "\n",
      "       [[ 199.,  224.,  243.],\n",
      "        [ 200.,  225.,  245.],\n",
      "        [ 199.,  225.,  244.],\n",
      "        ..., \n",
      "        [ 207.,  231.,  245.],\n",
      "        [ 203.,  230.,  245.],\n",
      "        [ 198.,  223.,  242.]],\n",
      "\n",
      "       ..., \n",
      "       [[ 113.,  102.,   84.],\n",
      "        [ 114.,  103.,   85.],\n",
      "        [ 114.,  103.,   85.],\n",
      "        ..., \n",
      "        [ 101.,   86.,   73.],\n",
      "        [ 102.,   88.,   74.],\n",
      "        [ 102.,   89.,   74.]],\n",
      "\n",
      "       [[ 116.,  105.,   87.],\n",
      "        [ 118.,  107.,   89.],\n",
      "        [ 119.,  108.,   90.],\n",
      "        ..., \n",
      "        [ 127.,  109.,   90.],\n",
      "        [ 125.,  108.,   90.],\n",
      "        [ 122.,  107.,   90.]],\n",
      "\n",
      "       [[ 121.,  110.,   92.],\n",
      "        [ 120.,  109.,   91.],\n",
      "        [ 120.,  109.,   90.],\n",
      "        ..., \n",
      "        [ 129.,  112.,   93.],\n",
      "        [ 128.,  112.,   92.],\n",
      "        [ 128.,  111.,   91.]]], dtype=float32), b'truck')\n",
      "(32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import traceback\n",
    "import sys\n",
    "import glob\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "directory= '/home/dp/down/cifar10/cifar-10-unpack/classify' #目标文件夹\n",
    "classlist=list()\n",
    "\n",
    "def read_cifar10():\n",
    "    train_dir=os.path.join(directory,'train')\n",
    "    classlist=os.listdir(train_dir)\n",
    "    \n",
    "    filenames=list()\n",
    "    labels=list()\n",
    "    for classes in classlist:\n",
    "        class_path=os.path.join(train_dir,classes)\n",
    "        filelist=os.listdir(class_path)\n",
    "        print('classname='+classes)\n",
    "        for file in filelist:\n",
    "            filefullName=os.path.join(class_path,file)\n",
    "            filenames.append(filefullName)\n",
    "            labels.append(classes)\n",
    "            #labels.append(classlist.index(classes))\n",
    "            #print(filefullName)       \n",
    "    #print(filenames,labels)    \n",
    "    tf_filenames=tf.constant(filenames)\n",
    "    tf_labels=tf.constant(labels)\n",
    "    # 此时dataset中的一个元素是(filename, label)\n",
    "    dataset = tf.contrib.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "\n",
    "    # 此时dataset中的一个元素是(image_resized, label)\n",
    "    dataset = dataset.map(_parse_function)\n",
    "\n",
    "    # 此时dataset中的一个元素是(image_resized_batch, label_batch)\n",
    "    #dataset = dataset.shuffle(buffer_size=50000).batch(32).repeat(10)\n",
    "    \n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    #batch_features, batch_labels = iterator.get_next()\n",
    "    # create TensorFlow Iterator object\n",
    "    #iterator = Iterator.from_structure(tr_data.output_types,\n",
    "    #                               tr_data.output_shapes)\n",
    "    next_element = iterator.get_next()\n",
    "    \n",
    "    # create two initialization ops to switch between the datasets\n",
    "    training_init_op = iterator.make_initializer(dataset)\n",
    "    \n",
    "\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        # initialize the iterator on the training data\n",
    "        sess.run(training_init_op)\n",
    "\n",
    "        # get each element of the training dataset until the end is reached\n",
    "        #while True:\n",
    "        try:\n",
    "            elem = sess.run(next_element)\n",
    "            print(elem)\n",
    "            print(elem[0].shape)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print(\"End of training dataset.\")\n",
    "           #break\n",
    "    \n",
    "# 函数的功能时将filename对应的图片文件读进来，并缩放到统一的大小\n",
    "def _parse_function(filename, label):\n",
    "    image_string = tf.read_file(filename)\n",
    "    image_decoded = tf.image.decode_png(image_string, channels=3)\n",
    "    image_resized = tf.image.resize_images(image_decoded, [32, 32])\n",
    "   \n",
    "    #one_hot = tf.one_hot(label, 10) #one_hot = tf.one_hot(label, NUM_CLASSES)\n",
    "    return image_resized, label\n",
    "    #return image_resized, one_hot\n",
    "read_cifar10()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
